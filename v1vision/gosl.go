// Code generated by "gosl"; DO NOT EDIT

package v1vision

import (
	"embed"
	"fmt"
	"math"
	"unsafe"
	"cogentcore.org/core/gpu"
	"cogentcore.org/lab/tensor"
)

//go:embed shaders/*.wgsl
var shaders embed.FS

var (
	// ComputeGPU is the compute gpu device
	ComputeGPU *gpu.GPU

	// UseGPU indicates whether to use GPU vs. CPU.
	UseGPU bool
)
// GPUSystem is a GPU compute System with kernels operating on the
// same set of data variables.
var GPUSystem *gpu.ComputeSystem

// GPUVars is an enum for GPU variables, for specifying what to sync.
type GPUVars int32 //enums:enum

const (
	CurOpVar GPUVars = 0
	KWTAsVar GPUVars = 1
	FiltersVar GPUVars = 2
	ImagesVar GPUVars = 3
	ValuesVar GPUVars = 4
	Values4DVar GPUVars = 5
	ScalarsVar GPUVars = 6
	InhibsVar GPUVars = 7
)

// Tensor stride variables
var TensorStrides tensor.Uint32

// GPUInit initializes the GPU compute system,
// configuring system(s), variables and kernels.
// It is safe to call multiple times: detects if already run.
func GPUInit() {
	if ComputeGPU != nil {
		return
	}
	gp := gpu.NewComputeGPU()
	ComputeGPU = gp
	_ = fmt.Sprintf("%g",math.NaN()) // keep imports happy
	{
		sy := gpu.NewComputeSystem(gp, "Default")
		GPUSystem = sy
		vars := sy.Vars()
		{
			sgp := vars.AddGroup(gpu.Storage, "Params")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("TensorStrides", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.AddStruct("CurOp", int(unsafe.Sizeof(Op{})), 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.AddStruct("KWTAs", int(unsafe.Sizeof(KWTA{})), 1, gpu.ComputeShader)
			vr.ReadOnly = true
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage, "Filters")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("Filters", gpu.Float32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage, "Data")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("Images", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Values", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Values4D", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Scalars", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Inhibs", gpu.Float32, 1, gpu.ComputeShader)
			sgp.SetNValues(1)
		}
		var pl *gpu.ComputePipeline
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/DoCurOp.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(1, "Filters")
		pl.AddVarUsed(2, "Images")
		pl.AddVarUsed(2, "Scalars")
		pl.AddVarUsed(2, "Values")
		pl.AddVarUsed(2, "Values4D")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/KWTAInitLayer.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Inhibs")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/KWTAInitPool.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Inhibs")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/KWTAIterLayerX.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Inhibs")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/KWTAIterLayerY.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Inhibs")
		pl.AddVarUsed(0, "KWTAs")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/KWTAIterPool.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Inhibs")
		pl.AddVarUsed(0, "KWTAs")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MaxScalarX.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MaxScalarY.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Scalars")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MeanScalarY.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Scalars")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MotionFullFieldX.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MotionFullFieldY.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Scalars")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/SumScalarX.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/SumScalarY.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Scalars")
		pl.AddVarUsed(2, "Values")
		sy.Config()
	}
}

// GPURelease releases the GPU compute system resources.
// Call this at program exit.
func GPURelease() {
	if GPUSystem != nil {
		GPUSystem.Release()
		GPUSystem = nil
	}

	if ComputeGPU != nil {
		ComputeGPU.Release()
		ComputeGPU = nil
	}
}

// RunDoCurOp runs the DoCurOp kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneDoCurOp call does Run and Done for a
// single run-and-sync case.
func RunDoCurOp(n int) {
	if UseGPU {
		RunDoCurOpGPU(n)
	} else {
		RunDoCurOpCPU(n)
	}
}

// RunDoCurOpGPU runs the DoCurOp kernel on the GPU. See [RunDoCurOp] for more info.
func RunDoCurOpGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["DoCurOp"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunDoCurOpCPU runs the DoCurOp kernel on the CPU.
func RunDoCurOpCPU(n int) {
	gpu.VectorizeFunc(0, n, DoCurOp)
}

// RunOneDoCurOp runs the DoCurOp kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneDoCurOp(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunDoCurOpGPU(n)
		RunDone(syncVars...)
	} else {
		RunDoCurOpCPU(n)
	}
}
// RunKWTAInitLayer runs the KWTAInitLayer kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneKWTAInitLayer call does Run and Done for a
// single run-and-sync case.
func RunKWTAInitLayer(n int) {
	if UseGPU {
		RunKWTAInitLayerGPU(n)
	} else {
		RunKWTAInitLayerCPU(n)
	}
}

// RunKWTAInitLayerGPU runs the KWTAInitLayer kernel on the GPU. See [RunKWTAInitLayer] for more info.
func RunKWTAInitLayerGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["KWTAInitLayer"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunKWTAInitLayerCPU runs the KWTAInitLayer kernel on the CPU.
func RunKWTAInitLayerCPU(n int) {
	gpu.VectorizeFunc(0, n, KWTAInitLayer)
}

// RunOneKWTAInitLayer runs the KWTAInitLayer kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneKWTAInitLayer(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunKWTAInitLayerGPU(n)
		RunDone(syncVars...)
	} else {
		RunKWTAInitLayerCPU(n)
	}
}
// RunKWTAInitPool runs the KWTAInitPool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneKWTAInitPool call does Run and Done for a
// single run-and-sync case.
func RunKWTAInitPool(n int) {
	if UseGPU {
		RunKWTAInitPoolGPU(n)
	} else {
		RunKWTAInitPoolCPU(n)
	}
}

// RunKWTAInitPoolGPU runs the KWTAInitPool kernel on the GPU. See [RunKWTAInitPool] for more info.
func RunKWTAInitPoolGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["KWTAInitPool"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunKWTAInitPoolCPU runs the KWTAInitPool kernel on the CPU.
func RunKWTAInitPoolCPU(n int) {
	gpu.VectorizeFunc(0, n, KWTAInitPool)
}

// RunOneKWTAInitPool runs the KWTAInitPool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneKWTAInitPool(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunKWTAInitPoolGPU(n)
		RunDone(syncVars...)
	} else {
		RunKWTAInitPoolCPU(n)
	}
}
// RunKWTAIterLayerX runs the KWTAIterLayerX kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneKWTAIterLayerX call does Run and Done for a
// single run-and-sync case.
func RunKWTAIterLayerX(n int) {
	if UseGPU {
		RunKWTAIterLayerXGPU(n)
	} else {
		RunKWTAIterLayerXCPU(n)
	}
}

// RunKWTAIterLayerXGPU runs the KWTAIterLayerX kernel on the GPU. See [RunKWTAIterLayerX] for more info.
func RunKWTAIterLayerXGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["KWTAIterLayerX"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunKWTAIterLayerXCPU runs the KWTAIterLayerX kernel on the CPU.
func RunKWTAIterLayerXCPU(n int) {
	gpu.VectorizeFunc(0, n, KWTAIterLayerX)
}

// RunOneKWTAIterLayerX runs the KWTAIterLayerX kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneKWTAIterLayerX(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunKWTAIterLayerXGPU(n)
		RunDone(syncVars...)
	} else {
		RunKWTAIterLayerXCPU(n)
	}
}
// RunKWTAIterLayerY runs the KWTAIterLayerY kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneKWTAIterLayerY call does Run and Done for a
// single run-and-sync case.
func RunKWTAIterLayerY(n int) {
	if UseGPU {
		RunKWTAIterLayerYGPU(n)
	} else {
		RunKWTAIterLayerYCPU(n)
	}
}

// RunKWTAIterLayerYGPU runs the KWTAIterLayerY kernel on the GPU. See [RunKWTAIterLayerY] for more info.
func RunKWTAIterLayerYGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["KWTAIterLayerY"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunKWTAIterLayerYCPU runs the KWTAIterLayerY kernel on the CPU.
func RunKWTAIterLayerYCPU(n int) {
	gpu.VectorizeFunc(0, n, KWTAIterLayerY)
}

// RunOneKWTAIterLayerY runs the KWTAIterLayerY kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneKWTAIterLayerY(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunKWTAIterLayerYGPU(n)
		RunDone(syncVars...)
	} else {
		RunKWTAIterLayerYCPU(n)
	}
}
// RunKWTAIterPool runs the KWTAIterPool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneKWTAIterPool call does Run and Done for a
// single run-and-sync case.
func RunKWTAIterPool(n int) {
	if UseGPU {
		RunKWTAIterPoolGPU(n)
	} else {
		RunKWTAIterPoolCPU(n)
	}
}

// RunKWTAIterPoolGPU runs the KWTAIterPool kernel on the GPU. See [RunKWTAIterPool] for more info.
func RunKWTAIterPoolGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["KWTAIterPool"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunKWTAIterPoolCPU runs the KWTAIterPool kernel on the CPU.
func RunKWTAIterPoolCPU(n int) {
	gpu.VectorizeFunc(0, n, KWTAIterPool)
}

// RunOneKWTAIterPool runs the KWTAIterPool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneKWTAIterPool(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunKWTAIterPoolGPU(n)
		RunDone(syncVars...)
	} else {
		RunKWTAIterPoolCPU(n)
	}
}
// RunMaxScalarX runs the MaxScalarX kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMaxScalarX call does Run and Done for a
// single run-and-sync case.
func RunMaxScalarX(n int) {
	if UseGPU {
		RunMaxScalarXGPU(n)
	} else {
		RunMaxScalarXCPU(n)
	}
}

// RunMaxScalarXGPU runs the MaxScalarX kernel on the GPU. See [RunMaxScalarX] for more info.
func RunMaxScalarXGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MaxScalarX"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMaxScalarXCPU runs the MaxScalarX kernel on the CPU.
func RunMaxScalarXCPU(n int) {
	gpu.VectorizeFunc(0, n, MaxScalarX)
}

// RunOneMaxScalarX runs the MaxScalarX kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMaxScalarX(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMaxScalarXGPU(n)
		RunDone(syncVars...)
	} else {
		RunMaxScalarXCPU(n)
	}
}
// RunMaxScalarY runs the MaxScalarY kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMaxScalarY call does Run and Done for a
// single run-and-sync case.
func RunMaxScalarY(n int) {
	if UseGPU {
		RunMaxScalarYGPU(n)
	} else {
		RunMaxScalarYCPU(n)
	}
}

// RunMaxScalarYGPU runs the MaxScalarY kernel on the GPU. See [RunMaxScalarY] for more info.
func RunMaxScalarYGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MaxScalarY"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMaxScalarYCPU runs the MaxScalarY kernel on the CPU.
func RunMaxScalarYCPU(n int) {
	gpu.VectorizeFunc(0, n, MaxScalarY)
}

// RunOneMaxScalarY runs the MaxScalarY kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMaxScalarY(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMaxScalarYGPU(n)
		RunDone(syncVars...)
	} else {
		RunMaxScalarYCPU(n)
	}
}
// RunMeanScalarY runs the MeanScalarY kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMeanScalarY call does Run and Done for a
// single run-and-sync case.
func RunMeanScalarY(n int) {
	if UseGPU {
		RunMeanScalarYGPU(n)
	} else {
		RunMeanScalarYCPU(n)
	}
}

// RunMeanScalarYGPU runs the MeanScalarY kernel on the GPU. See [RunMeanScalarY] for more info.
func RunMeanScalarYGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MeanScalarY"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMeanScalarYCPU runs the MeanScalarY kernel on the CPU.
func RunMeanScalarYCPU(n int) {
	gpu.VectorizeFunc(0, n, MeanScalarY)
}

// RunOneMeanScalarY runs the MeanScalarY kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMeanScalarY(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMeanScalarYGPU(n)
		RunDone(syncVars...)
	} else {
		RunMeanScalarYCPU(n)
	}
}
// RunMotionFullFieldX runs the MotionFullFieldX kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMotionFullFieldX call does Run and Done for a
// single run-and-sync case.
func RunMotionFullFieldX(n int) {
	if UseGPU {
		RunMotionFullFieldXGPU(n)
	} else {
		RunMotionFullFieldXCPU(n)
	}
}

// RunMotionFullFieldXGPU runs the MotionFullFieldX kernel on the GPU. See [RunMotionFullFieldX] for more info.
func RunMotionFullFieldXGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MotionFullFieldX"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMotionFullFieldXCPU runs the MotionFullFieldX kernel on the CPU.
func RunMotionFullFieldXCPU(n int) {
	gpu.VectorizeFunc(0, n, MotionFullFieldX)
}

// RunOneMotionFullFieldX runs the MotionFullFieldX kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMotionFullFieldX(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMotionFullFieldXGPU(n)
		RunDone(syncVars...)
	} else {
		RunMotionFullFieldXCPU(n)
	}
}
// RunMotionFullFieldY runs the MotionFullFieldY kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMotionFullFieldY call does Run and Done for a
// single run-and-sync case.
func RunMotionFullFieldY(n int) {
	if UseGPU {
		RunMotionFullFieldYGPU(n)
	} else {
		RunMotionFullFieldYCPU(n)
	}
}

// RunMotionFullFieldYGPU runs the MotionFullFieldY kernel on the GPU. See [RunMotionFullFieldY] for more info.
func RunMotionFullFieldYGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MotionFullFieldY"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMotionFullFieldYCPU runs the MotionFullFieldY kernel on the CPU.
func RunMotionFullFieldYCPU(n int) {
	gpu.VectorizeFunc(0, n, MotionFullFieldY)
}

// RunOneMotionFullFieldY runs the MotionFullFieldY kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMotionFullFieldY(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMotionFullFieldYGPU(n)
		RunDone(syncVars...)
	} else {
		RunMotionFullFieldYCPU(n)
	}
}
// RunSumScalarX runs the SumScalarX kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneSumScalarX call does Run and Done for a
// single run-and-sync case.
func RunSumScalarX(n int) {
	if UseGPU {
		RunSumScalarXGPU(n)
	} else {
		RunSumScalarXCPU(n)
	}
}

// RunSumScalarXGPU runs the SumScalarX kernel on the GPU. See [RunSumScalarX] for more info.
func RunSumScalarXGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["SumScalarX"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunSumScalarXCPU runs the SumScalarX kernel on the CPU.
func RunSumScalarXCPU(n int) {
	gpu.VectorizeFunc(0, n, SumScalarX)
}

// RunOneSumScalarX runs the SumScalarX kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneSumScalarX(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunSumScalarXGPU(n)
		RunDone(syncVars...)
	} else {
		RunSumScalarXCPU(n)
	}
}
// RunSumScalarY runs the SumScalarY kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneSumScalarY call does Run and Done for a
// single run-and-sync case.
func RunSumScalarY(n int) {
	if UseGPU {
		RunSumScalarYGPU(n)
	} else {
		RunSumScalarYCPU(n)
	}
}

// RunSumScalarYGPU runs the SumScalarY kernel on the GPU. See [RunSumScalarY] for more info.
func RunSumScalarYGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["SumScalarY"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunSumScalarYCPU runs the SumScalarY kernel on the CPU.
func RunSumScalarYCPU(n int) {
	gpu.VectorizeFunc(0, n, SumScalarY)
}

// RunOneSumScalarY runs the SumScalarY kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneSumScalarY(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunSumScalarYGPU(n)
		RunDone(syncVars...)
	} else {
		RunSumScalarYCPU(n)
	}
}
// RunDone must be called after Run* calls to start compute kernels.
// This actually submits the kernel jobs to the GPU, and adds commands
// to synchronize the given variables back from the GPU to the CPU.
// After this function completes, the GPU results will be available in 
// the specified variables.
func RunDone(syncVars ...GPUVars) {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	sy.ComputeEncoder.End()
	ReadFromGPU(syncVars...)
	sy.EndComputePass()
	SyncFromGPU(syncVars...)
}

// ToGPU copies given variables to the GPU for the system.
func ToGPU(vars ...GPUVars) {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case CurOpVar:
			v, _ := syVars.ValueByIndex(0, "CurOp", 0)
			gpu.SetValueFrom(v, CurOp)
		case KWTAsVar:
			v, _ := syVars.ValueByIndex(0, "KWTAs", 0)
			gpu.SetValueFrom(v, KWTAs)
		case FiltersVar:
			v, _ := syVars.ValueByIndex(1, "Filters", 0)
			gpu.SetValueFrom(v, Filters.Values)
		case ImagesVar:
			v, _ := syVars.ValueByIndex(2, "Images", 0)
			gpu.SetValueFrom(v, Images.Values)
		case ValuesVar:
			v, _ := syVars.ValueByIndex(2, "Values", 0)
			gpu.SetValueFrom(v, Values.Values)
		case Values4DVar:
			v, _ := syVars.ValueByIndex(2, "Values4D", 0)
			gpu.SetValueFrom(v, Values4D.Values)
		case ScalarsVar:
			v, _ := syVars.ValueByIndex(2, "Scalars", 0)
			gpu.SetValueFrom(v, Scalars.Values)
		case InhibsVar:
			v, _ := syVars.ValueByIndex(2, "Inhibs", 0)
			gpu.SetValueFrom(v, Inhibs.Values)
		}
	}
}
// RunGPUSync can be called to synchronize data between CPU and GPU.
// Any prior ToGPU* calls will execute to send data to the GPU,
// and any subsequent RunDone* calls will copy data back from the GPU.
func RunGPUSync() {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	sy.BeginComputePass()
}

// ToGPUTensorStrides gets tensor strides and starts copying to the GPU.
func ToGPUTensorStrides() {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	syVars := sy.Vars()
	TensorStrides.SetShapeSizes(60)
	TensorStrides.SetInt1D(Filters.Shape().Strides[0], 0)
	TensorStrides.SetInt1D(Filters.Shape().Strides[1], 1)
	TensorStrides.SetInt1D(Filters.Shape().Strides[2], 2)
	TensorStrides.SetInt1D(Filters.Shape().Strides[3], 3)
	TensorStrides.SetInt1D(Images.Shape().Strides[0], 10)
	TensorStrides.SetInt1D(Images.Shape().Strides[1], 11)
	TensorStrides.SetInt1D(Images.Shape().Strides[2], 12)
	TensorStrides.SetInt1D(Images.Shape().Strides[3], 13)
	TensorStrides.SetInt1D(Values.Shape().Strides[0], 20)
	TensorStrides.SetInt1D(Values.Shape().Strides[1], 21)
	TensorStrides.SetInt1D(Values.Shape().Strides[2], 22)
	TensorStrides.SetInt1D(Values.Shape().Strides[3], 23)
	TensorStrides.SetInt1D(Values.Shape().Strides[4], 24)
	TensorStrides.SetInt1D(Values4D.Shape().Strides[0], 30)
	TensorStrides.SetInt1D(Values4D.Shape().Strides[1], 31)
	TensorStrides.SetInt1D(Values4D.Shape().Strides[2], 32)
	TensorStrides.SetInt1D(Values4D.Shape().Strides[3], 33)
	TensorStrides.SetInt1D(Values4D.Shape().Strides[4], 34)
	TensorStrides.SetInt1D(Scalars.Shape().Strides[0], 40)
	TensorStrides.SetInt1D(Inhibs.Shape().Strides[0], 50)
	TensorStrides.SetInt1D(Inhibs.Shape().Strides[1], 51)
	TensorStrides.SetInt1D(Inhibs.Shape().Strides[2], 52)
	TensorStrides.SetInt1D(Inhibs.Shape().Strides[3], 53)
	v, _ := syVars.ValueByIndex(0, "TensorStrides", 0)
	gpu.SetValueFrom(v, TensorStrides.Values)
}

// ReadFromGPU starts the process of copying vars to the GPU.
func ReadFromGPU(vars ...GPUVars) {
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case CurOpVar:
			v, _ := syVars.ValueByIndex(0, "CurOp", 0)
			v.GPUToRead(sy.CommandEncoder)
		case KWTAsVar:
			v, _ := syVars.ValueByIndex(0, "KWTAs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case FiltersVar:
			v, _ := syVars.ValueByIndex(1, "Filters", 0)
			v.GPUToRead(sy.CommandEncoder)
		case ImagesVar:
			v, _ := syVars.ValueByIndex(2, "Images", 0)
			v.GPUToRead(sy.CommandEncoder)
		case ValuesVar:
			v, _ := syVars.ValueByIndex(2, "Values", 0)
			v.GPUToRead(sy.CommandEncoder)
		case Values4DVar:
			v, _ := syVars.ValueByIndex(2, "Values4D", 0)
			v.GPUToRead(sy.CommandEncoder)
		case ScalarsVar:
			v, _ := syVars.ValueByIndex(2, "Scalars", 0)
			v.GPUToRead(sy.CommandEncoder)
		case InhibsVar:
			v, _ := syVars.ValueByIndex(2, "Inhibs", 0)
			v.GPUToRead(sy.CommandEncoder)
		}
	}
}

// SyncFromGPU synchronizes vars from the GPU to the actual variable.
func SyncFromGPU(vars ...GPUVars) {
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case CurOpVar:
			v, _ := syVars.ValueByIndex(0, "CurOp", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, CurOp)
		case KWTAsVar:
			v, _ := syVars.ValueByIndex(0, "KWTAs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, KWTAs)
		case FiltersVar:
			v, _ := syVars.ValueByIndex(1, "Filters", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Filters.Values)
		case ImagesVar:
			v, _ := syVars.ValueByIndex(2, "Images", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Images.Values)
		case ValuesVar:
			v, _ := syVars.ValueByIndex(2, "Values", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Values.Values)
		case Values4DVar:
			v, _ := syVars.ValueByIndex(2, "Values4D", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Values4D.Values)
		case ScalarsVar:
			v, _ := syVars.ValueByIndex(2, "Scalars", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Scalars.Values)
		case InhibsVar:
			v, _ := syVars.ValueByIndex(2, "Inhibs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Inhibs.Values)
		}
	}
}

// GetCurOp returns a pointer to the given global variable: 
// [CurOp] []Op at given index. This directly processed in the GPU code,
// so this function call is an equivalent for the CPU.
func GetCurOp(idx uint32) *Op {
	return &CurOp[idx]
}

// GetKWTAs returns a pointer to the given global variable: 
// [KWTAs] []KWTA at given index. This directly processed in the GPU code,
// so this function call is an equivalent for the CPU.
func GetKWTAs(idx uint32) *KWTA {
	return &KWTAs[idx]
}
