// Code generated by "gosl"; DO NOT EDIT

package v1vision

import (
	"embed"
	"fmt"
	"math"
	"unsafe"
	"cogentcore.org/core/gpu"
	"cogentcore.org/lab/tensor"
)

//go:embed shaders/*.wgsl
var shaders embed.FS

var (
	// ComputeGPU is the compute gpu device
	ComputeGPU *gpu.GPU

	// UseGPU indicates whether to use GPU vs. CPU.
	UseGPU bool
)
// GPUSystem is a GPU compute System with kernels operating on the
// same set of data variables.
var GPUSystem *gpu.ComputeSystem

// GPUVars is an enum for GPU variables, for specifying what to sync.
type GPUVars int32 //enums:enum

const (
	CurOpVar GPUVars = 0
	KWTAsVar GPUVars = 1
	FiltersVar GPUVars = 2
	ImagesVar GPUVars = 3
	ValuesVar GPUVars = 4
	Values4DVar GPUVars = 5
	ScalarsVar GPUVars = 6
)

// Tensor stride variables
var TensorStrides tensor.Uint32

// GPUInit initializes the GPU compute system,
// configuring system(s), variables and kernels.
// It is safe to call multiple times: detects if already run.
func GPUInit() {
	if ComputeGPU != nil {
		return
	}
	gp := gpu.NewComputeGPU()
	ComputeGPU = gp
	_ = fmt.Sprintf("%g",math.NaN()) // keep imports happy
	{
		sy := gpu.NewComputeSystem(gp, "Default")
		GPUSystem = sy
		vars := sy.Vars()
		{
			sgp := vars.AddGroup(gpu.Storage, "Params")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("TensorStrides", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.AddStruct("CurOp", int(unsafe.Sizeof(Op{})), 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.AddStruct("KWTAs", int(unsafe.Sizeof(KWTA{})), 1, gpu.ComputeShader)
			vr.ReadOnly = true
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage, "Filters")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("Filters", gpu.Float32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage, "Data")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("Images", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Values", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Values4D", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Scalars", gpu.Float32, 1, gpu.ComputeShader)
			sgp.SetNValues(1)
		}
		var pl *gpu.ComputePipeline
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/DoCurOp.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(1, "Filters")
		pl.AddVarUsed(2, "Images")
		pl.AddVarUsed(2, "Scalars")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/KWTAInit.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Values")
		pl.AddVarUsed(2, "Values4D")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/KWTAIter.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(0, "KWTAs")
		pl.AddVarUsed(2, "Scalars")
		pl.AddVarUsed(2, "Values")
		pl.AddVarUsed(2, "Values4D")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MaxScalarP1.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MaxScalarP2.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Scalars")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MeanScalarP2.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Scalars")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MotionFullFieldP1.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MotionFullFieldP2.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Scalars")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/SumScalarP1.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Values")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/SumScalarP2.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(0, "CurOp")
		pl.AddVarUsed(2, "Scalars")
		pl.AddVarUsed(2, "Values")
		sy.Config()
	}
}

// GPURelease releases the GPU compute system resources.
// Call this at program exit.
func GPURelease() {
	if GPUSystem != nil {
		GPUSystem.Release()
		GPUSystem = nil
	}

	if ComputeGPU != nil {
		ComputeGPU.Release()
		ComputeGPU = nil
	}
}

// RunDoCurOp runs the DoCurOp kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneDoCurOp call does Run and Done for a
// single run-and-sync case.
func RunDoCurOp(n int) {
	if UseGPU {
		RunDoCurOpGPU(n)
	} else {
		RunDoCurOpCPU(n)
	}
}

// RunDoCurOpGPU runs the DoCurOp kernel on the GPU. See [RunDoCurOp] for more info.
func RunDoCurOpGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["DoCurOp"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunDoCurOpCPU runs the DoCurOp kernel on the CPU.
func RunDoCurOpCPU(n int) {
	gpu.VectorizeFunc(0, n, DoCurOp)
}

// RunOneDoCurOp runs the DoCurOp kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneDoCurOp(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunDoCurOpGPU(n)
		RunDone(syncVars...)
	} else {
		RunDoCurOpCPU(n)
	}
}
// RunKWTAInit runs the KWTAInit kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneKWTAInit call does Run and Done for a
// single run-and-sync case.
func RunKWTAInit(n int) {
	if UseGPU {
		RunKWTAInitGPU(n)
	} else {
		RunKWTAInitCPU(n)
	}
}

// RunKWTAInitGPU runs the KWTAInit kernel on the GPU. See [RunKWTAInit] for more info.
func RunKWTAInitGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["KWTAInit"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunKWTAInitCPU runs the KWTAInit kernel on the CPU.
func RunKWTAInitCPU(n int) {
	gpu.VectorizeFunc(0, n, KWTAInit)
}

// RunOneKWTAInit runs the KWTAInit kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneKWTAInit(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunKWTAInitGPU(n)
		RunDone(syncVars...)
	} else {
		RunKWTAInitCPU(n)
	}
}
// RunKWTAIter runs the KWTAIter kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneKWTAIter call does Run and Done for a
// single run-and-sync case.
func RunKWTAIter(n int) {
	if UseGPU {
		RunKWTAIterGPU(n)
	} else {
		RunKWTAIterCPU(n)
	}
}

// RunKWTAIterGPU runs the KWTAIter kernel on the GPU. See [RunKWTAIter] for more info.
func RunKWTAIterGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["KWTAIter"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunKWTAIterCPU runs the KWTAIter kernel on the CPU.
func RunKWTAIterCPU(n int) {
	gpu.VectorizeFunc(0, n, KWTAIter)
}

// RunOneKWTAIter runs the KWTAIter kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneKWTAIter(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunKWTAIterGPU(n)
		RunDone(syncVars...)
	} else {
		RunKWTAIterCPU(n)
	}
}
// RunMaxScalarP1 runs the MaxScalarP1 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMaxScalarP1 call does Run and Done for a
// single run-and-sync case.
func RunMaxScalarP1(n int) {
	if UseGPU {
		RunMaxScalarP1GPU(n)
	} else {
		RunMaxScalarP1CPU(n)
	}
}

// RunMaxScalarP1GPU runs the MaxScalarP1 kernel on the GPU. See [RunMaxScalarP1] for more info.
func RunMaxScalarP1GPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MaxScalarP1"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMaxScalarP1CPU runs the MaxScalarP1 kernel on the CPU.
func RunMaxScalarP1CPU(n int) {
	gpu.VectorizeFunc(0, n, MaxScalarP1)
}

// RunOneMaxScalarP1 runs the MaxScalarP1 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMaxScalarP1(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMaxScalarP1GPU(n)
		RunDone(syncVars...)
	} else {
		RunMaxScalarP1CPU(n)
	}
}
// RunMaxScalarP2 runs the MaxScalarP2 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMaxScalarP2 call does Run and Done for a
// single run-and-sync case.
func RunMaxScalarP2(n int) {
	if UseGPU {
		RunMaxScalarP2GPU(n)
	} else {
		RunMaxScalarP2CPU(n)
	}
}

// RunMaxScalarP2GPU runs the MaxScalarP2 kernel on the GPU. See [RunMaxScalarP2] for more info.
func RunMaxScalarP2GPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MaxScalarP2"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMaxScalarP2CPU runs the MaxScalarP2 kernel on the CPU.
func RunMaxScalarP2CPU(n int) {
	gpu.VectorizeFunc(0, n, MaxScalarP2)
}

// RunOneMaxScalarP2 runs the MaxScalarP2 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMaxScalarP2(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMaxScalarP2GPU(n)
		RunDone(syncVars...)
	} else {
		RunMaxScalarP2CPU(n)
	}
}
// RunMeanScalarP2 runs the MeanScalarP2 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMeanScalarP2 call does Run and Done for a
// single run-and-sync case.
func RunMeanScalarP2(n int) {
	if UseGPU {
		RunMeanScalarP2GPU(n)
	} else {
		RunMeanScalarP2CPU(n)
	}
}

// RunMeanScalarP2GPU runs the MeanScalarP2 kernel on the GPU. See [RunMeanScalarP2] for more info.
func RunMeanScalarP2GPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MeanScalarP2"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMeanScalarP2CPU runs the MeanScalarP2 kernel on the CPU.
func RunMeanScalarP2CPU(n int) {
	gpu.VectorizeFunc(0, n, MeanScalarP2)
}

// RunOneMeanScalarP2 runs the MeanScalarP2 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMeanScalarP2(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMeanScalarP2GPU(n)
		RunDone(syncVars...)
	} else {
		RunMeanScalarP2CPU(n)
	}
}
// RunMotionFullFieldP1 runs the MotionFullFieldP1 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMotionFullFieldP1 call does Run and Done for a
// single run-and-sync case.
func RunMotionFullFieldP1(n int) {
	if UseGPU {
		RunMotionFullFieldP1GPU(n)
	} else {
		RunMotionFullFieldP1CPU(n)
	}
}

// RunMotionFullFieldP1GPU runs the MotionFullFieldP1 kernel on the GPU. See [RunMotionFullFieldP1] for more info.
func RunMotionFullFieldP1GPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MotionFullFieldP1"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMotionFullFieldP1CPU runs the MotionFullFieldP1 kernel on the CPU.
func RunMotionFullFieldP1CPU(n int) {
	gpu.VectorizeFunc(0, n, MotionFullFieldP1)
}

// RunOneMotionFullFieldP1 runs the MotionFullFieldP1 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMotionFullFieldP1(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMotionFullFieldP1GPU(n)
		RunDone(syncVars...)
	} else {
		RunMotionFullFieldP1CPU(n)
	}
}
// RunMotionFullFieldP2 runs the MotionFullFieldP2 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMotionFullFieldP2 call does Run and Done for a
// single run-and-sync case.
func RunMotionFullFieldP2(n int) {
	if UseGPU {
		RunMotionFullFieldP2GPU(n)
	} else {
		RunMotionFullFieldP2CPU(n)
	}
}

// RunMotionFullFieldP2GPU runs the MotionFullFieldP2 kernel on the GPU. See [RunMotionFullFieldP2] for more info.
func RunMotionFullFieldP2GPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MotionFullFieldP2"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMotionFullFieldP2CPU runs the MotionFullFieldP2 kernel on the CPU.
func RunMotionFullFieldP2CPU(n int) {
	gpu.VectorizeFunc(0, n, MotionFullFieldP2)
}

// RunOneMotionFullFieldP2 runs the MotionFullFieldP2 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMotionFullFieldP2(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMotionFullFieldP2GPU(n)
		RunDone(syncVars...)
	} else {
		RunMotionFullFieldP2CPU(n)
	}
}
// RunSumScalarP1 runs the SumScalarP1 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneSumScalarP1 call does Run and Done for a
// single run-and-sync case.
func RunSumScalarP1(n int) {
	if UseGPU {
		RunSumScalarP1GPU(n)
	} else {
		RunSumScalarP1CPU(n)
	}
}

// RunSumScalarP1GPU runs the SumScalarP1 kernel on the GPU. See [RunSumScalarP1] for more info.
func RunSumScalarP1GPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["SumScalarP1"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunSumScalarP1CPU runs the SumScalarP1 kernel on the CPU.
func RunSumScalarP1CPU(n int) {
	gpu.VectorizeFunc(0, n, SumScalarP1)
}

// RunOneSumScalarP1 runs the SumScalarP1 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneSumScalarP1(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunSumScalarP1GPU(n)
		RunDone(syncVars...)
	} else {
		RunSumScalarP1CPU(n)
	}
}
// RunSumScalarP2 runs the SumScalarP2 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneSumScalarP2 call does Run and Done for a
// single run-and-sync case.
func RunSumScalarP2(n int) {
	if UseGPU {
		RunSumScalarP2GPU(n)
	} else {
		RunSumScalarP2CPU(n)
	}
}

// RunSumScalarP2GPU runs the SumScalarP2 kernel on the GPU. See [RunSumScalarP2] for more info.
func RunSumScalarP2GPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["SumScalarP2"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunSumScalarP2CPU runs the SumScalarP2 kernel on the CPU.
func RunSumScalarP2CPU(n int) {
	gpu.VectorizeFunc(0, n, SumScalarP2)
}

// RunOneSumScalarP2 runs the SumScalarP2 kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneSumScalarP2(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunSumScalarP2GPU(n)
		RunDone(syncVars...)
	} else {
		RunSumScalarP2CPU(n)
	}
}
// RunDone must be called after Run* calls to start compute kernels.
// This actually submits the kernel jobs to the GPU, and adds commands
// to synchronize the given variables back from the GPU to the CPU.
// After this function completes, the GPU results will be available in 
// the specified variables.
func RunDone(syncVars ...GPUVars) {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	sy.ComputeEncoder.End()
	ReadFromGPU(syncVars...)
	sy.EndComputePass()
	SyncFromGPU(syncVars...)
}

// ToGPU copies given variables to the GPU for the system.
func ToGPU(vars ...GPUVars) {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case CurOpVar:
			v, _ := syVars.ValueByIndex(0, "CurOp", 0)
			gpu.SetValueFrom(v, CurOp)
		case KWTAsVar:
			v, _ := syVars.ValueByIndex(0, "KWTAs", 0)
			gpu.SetValueFrom(v, KWTAs)
		case FiltersVar:
			v, _ := syVars.ValueByIndex(1, "Filters", 0)
			gpu.SetValueFrom(v, Filters.Values)
		case ImagesVar:
			v, _ := syVars.ValueByIndex(2, "Images", 0)
			gpu.SetValueFrom(v, Images.Values)
		case ValuesVar:
			v, _ := syVars.ValueByIndex(2, "Values", 0)
			gpu.SetValueFrom(v, Values.Values)
		case Values4DVar:
			v, _ := syVars.ValueByIndex(2, "Values4D", 0)
			gpu.SetValueFrom(v, Values4D.Values)
		case ScalarsVar:
			v, _ := syVars.ValueByIndex(2, "Scalars", 0)
			gpu.SetValueFrom(v, Scalars.Values)
		}
	}
}
// RunGPUSync can be called to synchronize data between CPU and GPU.
// Any prior ToGPU* calls will execute to send data to the GPU,
// and any subsequent RunDone* calls will copy data back from the GPU.
func RunGPUSync() {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	sy.BeginComputePass()
}

// ToGPUTensorStrides gets tensor strides and starts copying to the GPU.
func ToGPUTensorStrides() {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	syVars := sy.Vars()
	TensorStrides.SetShapeSizes(50)
	TensorStrides.SetInt1D(Filters.Shape().Strides[0], 0)
	TensorStrides.SetInt1D(Filters.Shape().Strides[1], 1)
	TensorStrides.SetInt1D(Filters.Shape().Strides[2], 2)
	TensorStrides.SetInt1D(Filters.Shape().Strides[3], 3)
	TensorStrides.SetInt1D(Images.Shape().Strides[0], 10)
	TensorStrides.SetInt1D(Images.Shape().Strides[1], 11)
	TensorStrides.SetInt1D(Images.Shape().Strides[2], 12)
	TensorStrides.SetInt1D(Images.Shape().Strides[3], 13)
	TensorStrides.SetInt1D(Values.Shape().Strides[0], 20)
	TensorStrides.SetInt1D(Values.Shape().Strides[1], 21)
	TensorStrides.SetInt1D(Values.Shape().Strides[2], 22)
	TensorStrides.SetInt1D(Values.Shape().Strides[3], 23)
	TensorStrides.SetInt1D(Values.Shape().Strides[4], 24)
	TensorStrides.SetInt1D(Values4D.Shape().Strides[0], 30)
	TensorStrides.SetInt1D(Values4D.Shape().Strides[1], 31)
	TensorStrides.SetInt1D(Values4D.Shape().Strides[2], 32)
	TensorStrides.SetInt1D(Values4D.Shape().Strides[3], 33)
	TensorStrides.SetInt1D(Values4D.Shape().Strides[4], 34)
	TensorStrides.SetInt1D(Scalars.Shape().Strides[0], 40)
	v, _ := syVars.ValueByIndex(0, "TensorStrides", 0)
	gpu.SetValueFrom(v, TensorStrides.Values)
}

// ReadFromGPU starts the process of copying vars to the GPU.
func ReadFromGPU(vars ...GPUVars) {
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case CurOpVar:
			v, _ := syVars.ValueByIndex(0, "CurOp", 0)
			v.GPUToRead(sy.CommandEncoder)
		case KWTAsVar:
			v, _ := syVars.ValueByIndex(0, "KWTAs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case FiltersVar:
			v, _ := syVars.ValueByIndex(1, "Filters", 0)
			v.GPUToRead(sy.CommandEncoder)
		case ImagesVar:
			v, _ := syVars.ValueByIndex(2, "Images", 0)
			v.GPUToRead(sy.CommandEncoder)
		case ValuesVar:
			v, _ := syVars.ValueByIndex(2, "Values", 0)
			v.GPUToRead(sy.CommandEncoder)
		case Values4DVar:
			v, _ := syVars.ValueByIndex(2, "Values4D", 0)
			v.GPUToRead(sy.CommandEncoder)
		case ScalarsVar:
			v, _ := syVars.ValueByIndex(2, "Scalars", 0)
			v.GPUToRead(sy.CommandEncoder)
		}
	}
}

// SyncFromGPU synchronizes vars from the GPU to the actual variable.
func SyncFromGPU(vars ...GPUVars) {
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case CurOpVar:
			v, _ := syVars.ValueByIndex(0, "CurOp", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, CurOp)
		case KWTAsVar:
			v, _ := syVars.ValueByIndex(0, "KWTAs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, KWTAs)
		case FiltersVar:
			v, _ := syVars.ValueByIndex(1, "Filters", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Filters.Values)
		case ImagesVar:
			v, _ := syVars.ValueByIndex(2, "Images", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Images.Values)
		case ValuesVar:
			v, _ := syVars.ValueByIndex(2, "Values", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Values.Values)
		case Values4DVar:
			v, _ := syVars.ValueByIndex(2, "Values4D", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Values4D.Values)
		case ScalarsVar:
			v, _ := syVars.ValueByIndex(2, "Scalars", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Scalars.Values)
		}
	}
}

// GetCurOp returns a pointer to the given global variable: 
// [CurOp] []Op at given index. This directly processed in the GPU code,
// so this function call is an equivalent for the CPU.
func GetCurOp(idx uint32) *Op {
	return &CurOp[idx]
}

// GetKWTAs returns a pointer to the given global variable: 
// [KWTAs] []KWTA at given index. This directly processed in the GPU code,
// so this function call is an equivalent for the CPU.
func GetKWTAs(idx uint32) *KWTA {
	return &KWTAs[idx]
}
